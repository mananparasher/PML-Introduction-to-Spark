{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to initialize Spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = pyspark.SparkConf().setAppName(\"Spark Tutorials\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating a  SparkContext object, we are telling Spark to access the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark works on resilient distributed dataset (RDD). \n",
    "RDD's can be partitioned across multipe nodes and operations can be done in parallel.\n",
    "RDD's are immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'Tutorials ',\n",
       " 'By ',\n",
       " 'Python',\n",
       " '-',\n",
       " 'Machine Learning',\n",
       " '.com',\n",
       " 'Spark',\n",
       " 'Is',\n",
       " 'Used']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sc.textFile(\"inputs.txt\")\n",
    "dataset.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " map(operation): Returns a new rdd after applying operation to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 1),\n",
       " ('Tutorials ', 1),\n",
       " ('By ', 1),\n",
       " ('Python', 1),\n",
       " ('-', 1),\n",
       " ('Machine Learning', 1),\n",
       " ('.com', 1),\n",
       " ('Spark', 1),\n",
       " ('Is', 1),\n",
       " ('Used', 1),\n",
       " ('For ', 1),\n",
       " ('Processing', 1),\n",
       " ('Big', 1),\n",
       " ('Data', 1),\n",
       " ('And', 1),\n",
       " ('Machine Learning', 1),\n",
       " ('In', 1),\n",
       " ('Various', 1),\n",
       " ('Industries.', 1)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_example=dataset.map(lambda x: (x,1))\n",
    "map_example.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter(operation) : Returns a new RDD after selecting the elements on which operation returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tutorials ',\n",
       " 'Python',\n",
       " 'Machine Learning',\n",
       " 'Processing',\n",
       " 'Machine Learning',\n",
       " 'Various',\n",
       " 'Industries.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_example=dataset.filter(lambda x: len(x)>5)\n",
    "filter_example.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupByKey(operation) : Returns a new RDD where the values for each key are grouped by the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', <pyspark.resultiterable.ResultIterable at 0x10e936350>),\n",
       " ('By ', <pyspark.resultiterable.ResultIterable at 0x10e936310>),\n",
       " ('Python', <pyspark.resultiterable.ResultIterable at 0x10e9364d0>),\n",
       " ('Machine Learning', <pyspark.resultiterable.ResultIterable at 0x10e936390>),\n",
       " ('.com', <pyspark.resultiterable.ResultIterable at 0x10e936590>),\n",
       " ('And', <pyspark.resultiterable.ResultIterable at 0x10e936610>),\n",
       " ('Various', <pyspark.resultiterable.ResultIterable at 0x10e936690>),\n",
       " ('Tutorials ', <pyspark.resultiterable.ResultIterable at 0x10e92c910>),\n",
       " ('-', <pyspark.resultiterable.ResultIterable at 0x10e92cb50>),\n",
       " ('Is', <pyspark.resultiterable.ResultIterable at 0x10e92c790>)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupByKey_example=map_example.groupByKey()\n",
    "groupByKey_example.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey(operation) : Returns a new RDD where the values for each key are reduced using the given operation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 2),\n",
       " ('By ', 1),\n",
       " ('Python', 1),\n",
       " ('Machine Learning', 2),\n",
       " ('.com', 1),\n",
       " ('And', 1),\n",
       " ('Various', 1),\n",
       " ('Tutorials ', 1),\n",
       " ('-', 1),\n",
       " ('Is', 1),\n",
       " ('Used', 1),\n",
       " ('For ', 1),\n",
       " ('Processing', 1),\n",
       " ('Big', 1),\n",
       " ('Data', 1),\n",
       " ('In', 1),\n",
       " ('Industries.', 1)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduceByKey_example = map_example.reduceByKey(lambda a, b: a + b)\n",
    "reduceByKey_example.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sortByKey(operation) : Returns a new RDD where the values for each key are sorted using the given sort function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 1),\n",
       " ('.com', 1),\n",
       " ('And', 1),\n",
       " ('Big', 1),\n",
       " ('By ', 1),\n",
       " ('Data', 1),\n",
       " ('For ', 1),\n",
       " ('In', 1),\n",
       " ('Industries.', 1),\n",
       " ('Is', 1),\n",
       " ('Machine Learning', 1),\n",
       " ('Machine Learning', 1),\n",
       " ('Processing', 1),\n",
       " ('Python', 1),\n",
       " ('Spark', 1),\n",
       " ('Spark', 1),\n",
       " ('Tutorials ', 1),\n",
       " ('Used', 1),\n",
       " ('Various', 1)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortByKey_example = map_example.sortByKey()\n",
    "sortByKey_example.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coalesce(number) : Returns an RDD with decreased number of partitions as given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoalescedRDD[203] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coalesce_example=map_example.coalesce(5)\n",
    "coalesce_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repartition(number) : Returns a new RDD with more or less number of partitions in a randomly shuffled manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[208] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repartition_example=map_example.repartition(1)\n",
    "repartition_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Functions in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saveAsTextFile : For saving a RDD as a textfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_example.saveAsTextFile(\"savefile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count : Returns the number of elements in a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_example=map_example.count()\n",
    "count_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " first : Returns the first element in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tutorials '"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_example.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take(n) : Returns an array of first n number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tutorials ', 'Python', 'Machine Learning', 'Processing', 'Machine Learning']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_example.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
